{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation as puncs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # word tokenization: spliting the snetence into words\n",
    "    words = [w for w in text.split(\" \")]\n",
    "    # making a set of all stop worked in english using nltk stopwords coupus\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    \n",
    "    trans = str.maketrans(\"\", \"\", puncs)\n",
    "    words = [w.translate(trans) for w in words]\n",
    "            \n",
    "    # removing stop words \n",
    "    words = [w for w in words if w not in stop_words]\n",
    "            \n",
    "    \n",
    "    # creating a list to get all the words after preprocessing\n",
    "    clean_sent = []\n",
    "    # making a WordNet lemmatizer's object using nltk\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    # creating the object of porter stemmer \n",
    "    ps = PorterStemmer()  \n",
    "    \n",
    "    #stemming and lemmatization\n",
    "    for w in words:\n",
    "        w = ps.stem(w)\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "        clean_sent.append(w)\n",
    "            \n",
    "    return clean_sent\n",
    "\n",
    "def get_synonyms(word):\n",
    "    return [s.name() for synonym in wordnet.synsets(word) for s in synonym.lemmas()]\n",
    "\n",
    "def count_simlilarity(word1, word2):\n",
    "    \n",
    "    try:\n",
    "        word1 = wordnet.synsets(word1)[0].name()\n",
    "        word2 = wordnet.synsets(word2)[0].name()\n",
    "        w1 = wordnet.synset(word1)\n",
    "        w2 = wordnet.synset(word2)\n",
    "        s = w1.wup_similarity(w2)\n",
    "        \n",
    "        if(s>0):\n",
    "            return w1.wup_similarity(w2)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Diabetes', 'Cardiovascular Disease', 'Chronic Obstructive Pulmonary Disease', 'Cancer']\n",
    "\n",
    "docList = []\n",
    "for cls in classes:\n",
    "    #reading the documents one by one\n",
    "    text = open(cls+\".txt\", \"r\").read()\n",
    "    \n",
    "    #preprocessing the text in each of the files\n",
    "    docList.append(preprocessing(text))    \n",
    "\n",
    "#removing all the duplicate tokens from the data\n",
    "for i in range(len(docList)):\n",
    "    docList[i] = list(set(docList[i]))\n",
    "    \n",
    "enhDocList = []\n",
    "for doc in docList:\n",
    "    enhDocList.append(list(set(doc + [w for word in doc for w in get_synonyms(word)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSim_classify(Query):\n",
    "\n",
    "    queryList = list(set(preprocessing(Query)))\n",
    "    enhQuery = queryList + list(set([w for word in queryList for w in get_synonyms(word)]))\n",
    "\n",
    "    similarity = [0, 0, 0, 0]\n",
    "    for word in queryList:\n",
    "        for i in range(len(docList)):\n",
    "            for w in docList[i]:\n",
    "                similarity[i] = similarity[i] + count_simlilarity(word, w)\n",
    "    similarity = [n/len(queryList) for n in similarity]\n",
    "\n",
    "    match_count= [0, 0, 0, 0]\n",
    "    for word in enhQuery:\n",
    "        for i in range(len(docList)):\n",
    "            for w in docList[i]:\n",
    "                if(word==w):\n",
    "                    match_count[i] = match_count[i] + 1\n",
    "    similarity = [x + y for x, y in zip(similarity, match_count)]\n",
    "    \n",
    "    doc_score_dic = {}\n",
    "    for i in range(len(similarity)):\n",
    "        doc_score_dic[i] = similarity[i]\n",
    "        \n",
    "    return doc_score_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"high blood pressure and heart pain\"\n",
    "d = wordSim_classify(Query)\n",
    "\n",
    "classes[max(d, key=d.get)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
