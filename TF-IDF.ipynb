{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation as puncs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # word tokenization: spliting the snetence into words\n",
    "    words = [w for w in text.split(\" \")]\n",
    "    # making a set of all stop worked in english using nltk stopwords coupus\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    \n",
    "    trans = str.maketrans(\"\", \"\", puncs)\n",
    "    words = [w.translate(trans) for w in words]\n",
    "            \n",
    "    # removing stop words \n",
    "    words = [w for w in words if w not in stop_words]\n",
    "            \n",
    "    \n",
    "    # creating a list to get all the words after preprocessing\n",
    "    clean_sent = []\n",
    "    # making a WordNet lemmatizer's object using nltk\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    # creating the object of porter stemmer \n",
    "    ps = PorterStemmer()  \n",
    "    \n",
    "    #stemming and lemmatization\n",
    "    for w in words:\n",
    "        w = ps.stem(w)\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "        clean_sent.append(w)\n",
    "            \n",
    "    return clean_sent\n",
    "\n",
    "### function to calculate term frequency in the doc\n",
    "def termFrequencyInDoc(wordList):\n",
    "#     \"\"\"\n",
    "#     This function should take a list of words as input argument, and output a dictionary of words such that\n",
    "#     each word that appears in the document is key in the dictionary and it's value is term frequency\n",
    "#     \"\"\"\n",
    "    termFrequency_dic={}\n",
    "    for w in wordList:\n",
    "        if w in termFrequency_dic.keys():\n",
    "            termFrequency_dic[w]+=1\n",
    "        else:\n",
    "            termFrequency_dic[w]=1\n",
    "    return termFrequency_dic\n",
    "\n",
    "\n",
    "\n",
    "# termFrequencyInDoc(removePuncs(wordList(txtFile)))\n",
    "def txtfToDictionary(txtFile):\n",
    "    text = open(txtFile, \"r\").read()\n",
    "    return termFrequencyInDoc(preprocessing(text))\n",
    "\n",
    "\n",
    "## function to calculate word Document frequency\n",
    "def wordDocFre(dicList):\n",
    "    vocan = {}\n",
    "    for docDic in dicList:\n",
    "        for keys in docDic.keys():\n",
    "            if keys in vocan.keys():\n",
    "                ## add some code here\n",
    "                vocan[keys]+=1\n",
    "            else:\n",
    "                ## add some code here\n",
    "                vocan[keys]=1\n",
    "    return vocan\n",
    "\n",
    "\n",
    "## construct a function named inverseDocFre() that takes dictionary returned from wordDocFre functions above\n",
    "## and outputs inverse document frequency of each word. You can do it!\n",
    "def invrDocFre(dic,M,Base):\n",
    "    dictidf = {}\n",
    "    for keys in dic.keys():\n",
    "        if(dic[keys]!=0):\n",
    "            dictidf[keys]= math.log((float(M+1)/dic[keys]),Base)\n",
    "        else:\n",
    "            print(keys+\"  \"+str(dic[keys]))\n",
    "    return dictidf\n",
    "\n",
    "\n",
    "\n",
    "### this function will calculate tf-idf for everyword in doc\n",
    "## this is the main function which calls the above functions\n",
    "def tfidf(list_of_doc_dic,idf_dic): \n",
    "    #first input is the list of all disctionaries after Punctuations have been removed\n",
    "    \n",
    "    list_of_tfidf_dic=[] #this contains tfidf dictionaries for each document\n",
    "    for dic in list_of_doc_dic:\n",
    "        tfidf_dic = {}\n",
    "        for keys in dic.keys():\n",
    "            tfidf_dic[keys] = dic[keys] * idf_dic[keys]\n",
    "        list_of_tfidf_dic.append(tfidf_dic)\n",
    "        \n",
    "    return list_of_tfidf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = ['Diabetes', 'Cardiovascular Disease', 'Chronic Obstructive Pulmonary Disease', 'Cancer']\n",
    "\n",
    "dicList = []\n",
    "for cls in classes:\n",
    "    dicList.append(txtfToDictionary(cls+\".txt\"))\n",
    "    \n",
    "WDF_dict = wordDocFre(dicList)\n",
    "IDF_dict = invrDocFre(WDF_dict,len(classes),10)\n",
    "TFIDF_dic_List = tfidf(dicList,IDF_dict)\n",
    "\n",
    "Doc_V = []\n",
    "Vocabulary = np.transpose(np.array(pd.Series(IDF_dict).index))\n",
    "for docdic in TFIDF_dic_List:\n",
    "    Document_V = np.zeros(len(Vocabulary))\n",
    "    for keys in docdic.keys():\n",
    "        Document_V[np.where(Vocabulary == keys)] = docdic[keys]\n",
    "    Doc_V.append(Document_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_classify(Query):\n",
    "\n",
    "    doc_score_dic = {}\n",
    "    Vocabulary = np.transpose(np.array(pd.Series(IDF_dict).index))\n",
    "    Query_V = np.zeros(len(Vocabulary))\n",
    "\n",
    "    words_q = preprocessing(Query)    \n",
    "\n",
    "    for i in words_q:\n",
    "        Query_V[np.where(Vocabulary == i)] = 1\n",
    "\n",
    "    count = 0\n",
    "    for V in Doc_V:\n",
    "        doc_score_dic[count] = np.dot(Query_V, V)\n",
    "        count = count + 1\n",
    "    \n",
    "    return doc_score_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = \"high blood pressure and heart pain\"\n",
    "d = tfidf_classify(Query)\n",
    "\n",
    "classes[max(d, key=d.get)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
